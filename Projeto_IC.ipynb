{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTnJjpcVhlXg"
   },
   "source": [
    "# Projeto IC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77jQJFN5hrCi"
   },
   "source": [
    "Download e import das bibliotecas necessárias para a execução das rotinas em python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA-dui6Dh038",
    "outputId": "f87d8c0e-7799-4ecf-f6d2-fff254b916da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.11/site-packages (10.0.1)\n",
      "Requirement already satisfied: h5py in /opt/homebrew/lib/python3.11/site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/lib/python3.11/site-packages (from h5py) (1.24.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36lSrdMClgjF"
   },
   "source": [
    "Código construído na primeira fase da IC\n",
    "\n",
    "O código abaixo realiza a leitura de um arquivo .mrst gerado pelo software GMSH, extrai os dados, as entidades e os elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QUT4klDGhrP0"
   },
   "outputs": [],
   "source": [
    "class MSHFileParse:\n",
    "    def __init__ (self, filename):\n",
    "        #buffer\n",
    "        with open(filename, 'r') as file:\n",
    "            buffer = file.read()\n",
    "\n",
    "        # minimal sections that should be present\n",
    "        self.meshFormat = MeshFormat(buffer)\n",
    "        self.nodes = Nodes(buffer)\n",
    "        self.elements = Elements(buffer)\n",
    "        self.entities = Entities(buffer)\n",
    "\n",
    "\n",
    "class MeshFormat:\n",
    "    def __init__ (self, buffer):\n",
    "        self.version = None\n",
    "        self.fileType = None\n",
    "        self.dataSize = None\n",
    "\n",
    "        self.extractMeshFormat(buffer)\n",
    "        self.to_hdf5('mesh_format.h5')\n",
    "\n",
    "    def extractMeshFormat (self, buffer):\n",
    "        match = re.search(r'\\$MeshFormat\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)\\s+\\$EndMeshFormat', buffer)\n",
    "        if match:\n",
    "            self.version = float(match.group(1))\n",
    "            self.fileType = int(match.group(2))\n",
    "            self.dataSize = int(match.group(3))\n",
    "\n",
    "            # delete the mesh format section on buffer\n",
    "            endIndex = buffer.find('$EndMeshFormat') + len('$EndMeshFormat')\n",
    "            buffer = buffer[endIndex:]\n",
    "            # print(buffer)\n",
    "        else:\n",
    "            raise Exception('MeshFormat not found or invalid')\n",
    "\n",
    "    def to_hdf5(self, filename):\n",
    "        with h5py.File(filename, 'w') as file:\n",
    "            file.create_dataset('version', data=self.version)\n",
    "            file.create_dataset('fileType', data=self.fileType)\n",
    "            file.create_dataset('dataSize', data=self.dataSize)\n",
    "\n",
    "        print(f'Mesh format saved to {filename}')\n",
    "\n",
    "class Entities:\n",
    "    def __init__ (self, buffer):\n",
    "        self.numEntityBlocksPoints = None\n",
    "        self.numEntityBlocksCurves = None\n",
    "        self.numEntityBlocksSurfaces = None\n",
    "        self.numEntityBlocksVolumes = None\n",
    "        self.entityPoints = []\n",
    "        self.entityCurves = []\n",
    "        self.entitySurfaces = []\n",
    "        self.entityVolumes = []\n",
    "\n",
    "        self.extractEntities(buffer)\n",
    "        self.toHdf5('entities.h5')\n",
    "\n",
    "    def extractEntities(self, buffer):\n",
    "        match = re.search(r'\\$Entities\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s', buffer)\n",
    "        if match:\n",
    "            self.numEntityBlocksPoints = int(match.group(1))\n",
    "            self.numEntityBlocksCurves = int(match.group(2))\n",
    "            self.numEntityBlocksSurfaces = int(match.group(3))\n",
    "            self.numEntityBlocksVolumes = int(match.group(4))\n",
    "            buffer = buffer[match.end():]\n",
    "            # print(buffer)\n",
    "\n",
    "            # loop over points\n",
    "            for _ in range(self.numEntityBlocksPoints):\n",
    "                match = re.search(r'(\\d+)\\s(.+)\\s', buffer)\n",
    "                if match:\n",
    "                    entity_point = {\n",
    "                        'pointTag': int(match.group(1)),\n",
    "                        'pointCoord': [float(match.group(2).split()[i]) for i in range(3)],\n",
    "                        'numPhysicalTags': int(match.group(2).split()[3]),\n",
    "                        'physicalTags': []\n",
    "                    }\n",
    "\n",
    "                    # loop over physical tags\n",
    "                    for _ in range(entity_point['numPhysicalTags']):\n",
    "                        entity_point['physicalTags'].append(int(match.group(2).split()[4 + _]))\n",
    "\n",
    "                    self.entityPoints.append(entity_point)\n",
    "                    buffer = buffer[match.end():]\n",
    "\n",
    "            # loop over curves\n",
    "            for _ in range(self.numEntityBlocksCurves):\n",
    "                match = re.search(r'(\\d+)\\s(.+)\\s', buffer)\n",
    "                if match:\n",
    "                    entity_curve = {\n",
    "                        'curveTag': int(match.group(1)),\n",
    "                        'curveCoords': [float(match.group(2).split()[i]) for i in range(6)],\n",
    "                        'numPhysicalTags': int(match.group(2).split()[6]),\n",
    "                        'physicalTags': []\n",
    "                    }\n",
    "\n",
    "                    # loop over physical tags\n",
    "                    for _ in range(entity_curve['numPhysicalTags']):\n",
    "                        entity_curve['physicalTags'].append(int(match.group(2).split()[7 + _]))\n",
    "\n",
    "                    entity_curve['numBoundingPoints'] = int(match.group(2).split()[7 + entity_curve['numPhysicalTags']])\n",
    "                    entity_curve['PointTag'] = []\n",
    "\n",
    "                    # loop over bounding points\n",
    "                    for _ in range(entity_curve['numBoundingPoints']):\n",
    "                        entity_curve['PointTag'].append(int(match.group(2).split()[8 + entity_curve['numPhysicalTags'] + _]))\n",
    "\n",
    "                    self.entityCurves.append(entity_curve)\n",
    "                    buffer = buffer[match.end():]\n",
    "\n",
    "            # loop over surfaces\n",
    "            for _ in range(self.numEntityBlocksSurfaces):\n",
    "                match = re.search(r'(\\d+)\\s(.+)\\s', buffer)\n",
    "                if match:\n",
    "                    entity_surface = {\n",
    "                        'surfaceTag': int(match.group(1)),\n",
    "                        'surfaceCoords': [float(match.group(2).split()[i]) for i in range(6)],\n",
    "                        'numPhysicalTags': int(match.group(2).split()[6]),\n",
    "                        'physicalTags': []\n",
    "                    }\n",
    "\n",
    "                    # loop over physical tags\n",
    "                    for i in range(entity_surface['numPhysicalTags']):\n",
    "                        entity_surface['physicalTags'].append(int(match.group(2).split()[7 + i]))\n",
    "\n",
    "                    entity_surface['numBoundingCurves'] = int(match.group(2).split()[7 + entity_surface['numPhysicalTags']])\n",
    "                    entity_surface['CurveTag'] = []\n",
    "\n",
    "                    # loop over bounding curves\n",
    "                    for _ in range(entity_surface['numBoundingCurves']):\n",
    "                        entity_surface['CurveTag'].append(int(match.group(2).split()[8 + entity_surface['numPhysicalTags'] + _]))\n",
    "\n",
    "                    self.entitySurfaces.append(entity_surface)\n",
    "                    buffer = buffer[match.end():]\n",
    "\n",
    "            # loop over volumes\n",
    "            for _ in range(self.numEntityBlocksVolumes):\n",
    "                match = re.search(r'(\\d+)\\s(.+)\\s', buffer)\n",
    "                if match:\n",
    "                    entity_volume = {\n",
    "                        'volumeTag': int(match.group(1)),\n",
    "                        'volumeCoords': [float(match.group(2).split()[i]) for i in range(6)],\n",
    "                        'numPhysicalTags': int(match.group(2).split()[6]),\n",
    "                        'physicalTags': []\n",
    "                    }\n",
    "\n",
    "                    # loop over physical tags\n",
    "                    for _ in range(entity_volume['numPhysicalTags']):\n",
    "                        entity_volume['physicalTags'].append(int(match.group(2).split()[7 + _]))\n",
    "\n",
    "                    entity_volume['numBoundingSurfaces'] = int(match.group(2).split()[7 + entity_volume['numPhysicalTags']])\n",
    "                    entity_volume['SurfaceTag'] = []\n",
    "\n",
    "                    # loop over bounding surfaces\n",
    "                    for _ in range(entity_volume['numBoundingSurfaces']):\n",
    "                        entity_volume['SurfaceTag'].append(int(match.group(2).split()[8 + entity_volume['numPhysicalTags'] + _]))\n",
    "\n",
    "                    self.entityVolumes.append(entity_volume)\n",
    "                    buffer = buffer[match.end():]\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise Exception('Entities not found or invalid')\n",
    "\n",
    "    def toNumpy(self):\n",
    "        #points\n",
    "        pointsTags = [np.array(block['physicalTags']) for block in self.entityPoints]\n",
    "\n",
    "        #curves\n",
    "        curvesTags = [np.array(block['physicalTags']) for block in self.entityCurves]\n",
    "        curvesBoundingPoints = [np.array(block['PointTag']) for block in self.entityCurves]\n",
    "\n",
    "        #surfaces\n",
    "        surfacesTags = [np.array(block['physicalTags']) for block in self.entitySurfaces]\n",
    "        surfacesCurves = [np.array(block['CurveTag']) for block in self.entitySurfaces]\n",
    "\n",
    "        #volumes\n",
    "        volumesTags = [np.array(block['physicalTags']) for block in self.entityVolumes]\n",
    "        volumesSurfaces = [np.array(block['SurfaceTag']) for block in self.entityVolumes]\n",
    "\n",
    "        return pointsTags, (curvesTags, curvesBoundingPoints), (surfacesTags, surfacesCurves), (volumesTags, volumesSurfaces)\n",
    "\n",
    "\n",
    "    def toHdf5(self, filename):\n",
    "        pointsTags, (curvesTags, curvesBoundingPoints), (surfacesTags, surfacesCurves), (volumesTags, volumesSurfaces) = self.toNumpy()\n",
    "\n",
    "        with h5py.File(filename, 'w') as file:\n",
    "            # Saving general information\n",
    "            file.create_dataset('numEntityBlocksPoints', data=self.numEntityBlocksPoints)\n",
    "            file.create_dataset('numEntityBlocksCurves', data=self.numEntityBlocksCurves)\n",
    "            file.create_dataset('numEntityBlocksSurfaces', data=self.numEntityBlocksSurfaces)\n",
    "            file.create_dataset('numEntityBlocksVolumes', data=self.numEntityBlocksVolumes)\n",
    "\n",
    "            # Saving entity blocks points\n",
    "            for i, (tags) in enumerate(zip(pointsTags)):\n",
    "                group = file.create_group(f'entityBlockPoints{i}')\n",
    "                group.create_dataset('pointTag', data=self.entityPoints[i]['pointTag'])\n",
    "                group.create_dataset('pointCoord', data=self.entityPoints[i]['pointCoord'])\n",
    "                group.create_dataset('numPhysicalTags', data=self.entityPoints[i]['numPhysicalTags'])\n",
    "                group.create_dataset('physicalTags', data=tags)\n",
    "\n",
    "            # Saving entity blocks curves\n",
    "            for i, (tags, boundingPoints) in enumerate(zip(curvesTags, curvesBoundingPoints)):\n",
    "                group = file.create_group(f'entityBlockCurves{i}')\n",
    "                group.create_dataset('curveTag', data=self.entityCurves[i]['curveTag'])\n",
    "                group.create_dataset('curveCoords', data=self.entityCurves[i]['curveCoords'])\n",
    "                group.create_dataset('numPhysicalTags', data=self.entityCurves[i]['numPhysicalTags'])\n",
    "                group.create_dataset('physicalTags', data=tags)\n",
    "                group.create_dataset('numBoundingPoints', data=self.entityCurves[i]['numBoundingPoints'])\n",
    "                group.create_dataset('pointTag', data=boundingPoints)\n",
    "\n",
    "            # Saving entity blocks surfaces\n",
    "            for i, (tags, curves) in enumerate(zip(surfacesTags, surfacesCurves)):\n",
    "                group = file.create_group(f'entityBlockSurfaces{i}')\n",
    "                group.create_dataset('surfaceTag', data=self.entitySurfaces[i]['surfaceTag'])\n",
    "                group.create_dataset('surfaceCoords', data=self.entitySurfaces[i]['surfaceCoords'])\n",
    "                group.create_dataset('numPhysicalTags', data=self.entitySurfaces[i]['numPhysicalTags'])\n",
    "                group.create_dataset('physicalTags', data=tags)\n",
    "                group.create_dataset('numBoundingCurves', data=self.entitySurfaces[i]['numBoundingCurves'])\n",
    "                group.create_dataset('curveTag', data=curves)\n",
    "\n",
    "            # Saving entity blocks volumes\n",
    "            for i, (tags, surfaces) in enumerate(zip(volumesTags, volumesSurfaces)):\n",
    "                group = file.create_group(f'entityBlockVolumes{i}')\n",
    "                group.create_dataset('volumeTag', data=self.entityVolumes[i]['volumeTag'])\n",
    "                group.create_dataset('volumeCoords', data=self.entityVolumes[i]['volumeCoords'])\n",
    "                group.create_dataset('numPhysicalTags', data=self.entityVolumes[i]['numPhysicalTags'])\n",
    "                group.create_dataset('physicalTags', data=tags)\n",
    "                group.create_dataset('numBoundingSurfaces', data=self.entityVolumes[i]['numBoundingSurfaces'])\n",
    "                group.create_dataset('surfaceTag', data=surfaces)\n",
    "\n",
    "        print(f'Entities saved to {filename}')\n",
    "\n",
    "\n",
    "class Nodes:\n",
    "    def __init__ (self, buffer):\n",
    "        self.numEntityBlocks = None\n",
    "        self.numNodes = None\n",
    "        self.minNodeTag = None\n",
    "        self.maxNodeTag = None\n",
    "        self.entityBlock = []\n",
    "\n",
    "        self.extractNodes(buffer)\n",
    "        self.toHdf5('nodes.h5')\n",
    "\n",
    "    def extractNodes(self, buffer):\n",
    "        match = re.search(r'\\$Nodes\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s', buffer)\n",
    "        if match:\n",
    "            # general information\n",
    "            self.numEntityBlocks = int(match.group(1))\n",
    "            self.numNodes = int(match.group(2))\n",
    "            self.minNodeTag = int(match.group(3))\n",
    "            self.maxNodeTag = int(match.group(4))\n",
    "            buffer = buffer[match.end():]\n",
    "            # print(buffer)\n",
    "\n",
    "            # loop over entity blocks\n",
    "            for _ in range(self.numEntityBlocks):\n",
    "                match = re.search(r'(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s', buffer)\n",
    "                if match:\n",
    "                    entity_block = {\n",
    "                        'entityDim': int(match.group(1)),\n",
    "                        'entityTag': int(match.group(2)),\n",
    "                        'parametric': int(match.group(3)),\n",
    "                        'numNodesInBlock': int(match.group(4)),\n",
    "                        'nodeTag': [],\n",
    "                        'nodeCoord': []\n",
    "                    }\n",
    "                    buffer = buffer[match.end():]\n",
    "\n",
    "                    # loop over nodes in block\n",
    "                    # fisrt all the tags\n",
    "                    for _ in range(entity_block['numNodesInBlock']):\n",
    "                        match = re.search(r'(\\d+)\\s', buffer)\n",
    "                        if match:\n",
    "                            node_tag = int(match.group(1))\n",
    "                            entity_block['nodeTag'].append(node_tag)\n",
    "                            buffer = buffer[match.end():]\n",
    "\n",
    "                    # then all the coordinates\n",
    "                    for _ in range(entity_block['numNodesInBlock']):\n",
    "                        match = re.search(r'(.+)\\s', buffer)\n",
    "                        if match:\n",
    "                            node_coord = [float(coord) for coord in match.group(1).split()]\n",
    "                            entity_block['nodeCoord'].append(node_coord)\n",
    "                            buffer = buffer[match.end():]\n",
    "                        else:\n",
    "                            raise Exception('Node coordinates not found or invalid')\n",
    "\n",
    "                    self.entityBlock.append(entity_block)\n",
    "\n",
    "        else:\n",
    "            raise Exception('Nodes not found or invalid')\n",
    "\n",
    "    def toNumpy(self):\n",
    "        nodeTags = [np.array(block['nodeTag']) for block in self.entityBlock]\n",
    "        nodeCoords = [np.array(block['nodeCoord']) for block in self.entityBlock]\n",
    "\n",
    "        return nodeTags, nodeCoords\n",
    "\n",
    "    def toHdf5(self, filename):\n",
    "        nodeTags, nodeCoords = self.toNumpy()\n",
    "\n",
    "        with h5py.File(filename, 'w') as file:\n",
    "            # Saving general information\n",
    "            file.create_dataset('numEntityBlocks', data=self.numEntityBlocks)\n",
    "            file.create_dataset('numNodes', data=self.numNodes)\n",
    "            file.create_dataset('minNodeTag', data=self.minNodeTag)\n",
    "            file.create_dataset('maxNodeTag', data=self.maxNodeTag)\n",
    "\n",
    "            # Saving entity blocks\n",
    "            for i, (tags, coords) in enumerate(zip(nodeTags, nodeCoords)):\n",
    "                group = file.create_group(f'entityBlock{i}')\n",
    "                group.create_dataset('entityDim', data=self.entityBlock[i]['entityDim'])\n",
    "                group.create_dataset('entityTag', data=self.entityBlock[i]['entityTag'])\n",
    "                group.create_dataset('parametric', data=self.entityBlock[i]['parametric'])\n",
    "                group.create_dataset('numNodesInBlock', data=self.entityBlock[i]['numNodesInBlock'])\n",
    "                group.create_dataset('nodeTags', data=tags)\n",
    "                group.create_dataset('nodeCoords', data=coords)\n",
    "\n",
    "        print(f'Nodes saved to {filename}')\n",
    "\n",
    "\n",
    "class Elements:\n",
    "    def __init__(self, buffer):\n",
    "        self.numEntityBlocks = None\n",
    "        self.numElements = None\n",
    "        self.minElementTag = None\n",
    "        self.maxElementTag = None\n",
    "        self.entityBlock = []\n",
    "\n",
    "        self.extractElements(buffer)\n",
    "        self.toHdf5('elements.h5')\n",
    "\n",
    "    def extractElements(self, buffer):\n",
    "        match = re.search(r'\\$Elements\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s', buffer)\n",
    "        if match:\n",
    "            # general information (first line)\n",
    "            self.numEntityBlocks = int(match.group(1))\n",
    "            self.numElements = int(match.group(2))\n",
    "            self.minElementTag = int(match.group(3))\n",
    "            self.maxElementTag = int(match.group(4))\n",
    "            buffer = buffer[match.end():]\n",
    "\n",
    "            # loop over entity blocks\n",
    "            for _ in range(self.numEntityBlocks):\n",
    "                match = re.search(r'(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s', buffer)\n",
    "                if match:\n",
    "                    entity_block = {\n",
    "                        'entityDim': int(match.group(1)),\n",
    "                        'entityTag': int(match.group(2)),\n",
    "                        'elementType': int(match.group(3)),\n",
    "                        'numElementsInBlock': int(match.group(4)),\n",
    "                        'elementTags': [],\n",
    "                        'elementNodes': []\n",
    "                    }\n",
    "                    buffer = buffer[match.end():]\n",
    "\n",
    "                    # loop over elements in block\n",
    "                    for _ in range(entity_block['numElementsInBlock']):\n",
    "                        match = re.search(r'(\\d+)\\s(.+)\\s', buffer)\n",
    "                        if match:\n",
    "                            element_tag = int(match.group(1))\n",
    "                            element_nodes = [int(node) for node in match.group(2).split()]\n",
    "                            entity_block['elementTags'].append(element_tag)\n",
    "                            entity_block['elementNodes'].append(element_nodes)\n",
    "                            buffer = buffer[match.end():]\n",
    "\n",
    "                    self.entityBlock.append(entity_block)\n",
    "        else:\n",
    "            raise Exception('Elements not found or invalid')\n",
    "\n",
    "    def toNumpy(self):\n",
    "        elementTags = [np.array(block['elementTags']) for block in self.entityBlock]\n",
    "        elementNodes = [np.array(block['elementNodes']) for block in self.entityBlock]\n",
    "\n",
    "        return elementTags, elementNodes\n",
    "\n",
    "    def toHdf5(self, filename):\n",
    "        elementTags, elementNodes = self.toNumpy()\n",
    "\n",
    "        with h5py.File(filename, 'w') as file:\n",
    "            # Saving general information\n",
    "            file.create_dataset('numEntityBlocks', data=self.numEntityBlocks)\n",
    "            file.create_dataset('numElements', data=self.numElements)\n",
    "            file.create_dataset('minElementTag', data=self.minElementTag)\n",
    "            file.create_dataset('maxElementTag', data=self.maxElementTag)\n",
    "\n",
    "            # Saving entity blocks\n",
    "            for i, (tags, nodes) in enumerate(zip(elementTags, elementNodes)):\n",
    "                group = file.create_group(f'entityBlock{i}')\n",
    "                group.create_dataset('entityDim', data=self.entityBlock[i]['entityDim'])\n",
    "                group.create_dataset('entityTag', data=self.entityBlock[i]['entityTag'])\n",
    "                group.create_dataset('elementType', data=self.entityBlock[i]['elementType'])\n",
    "                group.create_dataset('numElementsInBlock', data=self.entityBlock[i]['numElementsInBlock'])\n",
    "                group.create_dataset('elementTags', data=tags)\n",
    "                group.create_dataset('elementNodes', data=nodes)\n",
    "\n",
    "        print(f'Elements saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeshDataStructure:\n",
    "  def __init__(self, nodes_file: str, connectivities_file: str):\n",
    "    self.nodes_file = nodes_file\n",
    "    self.connectivities_file = connectivities_file\n",
    "    self.nodes = None\n",
    "    self.connectivities = None\n",
    "    self.centroids = None\n",
    "    self.coords_centroids = None\n",
    "    self.nHorizontal = None\n",
    "    self.nVertical = None\n",
    "    self.coord_grossa = None\n",
    "    self.grossa_centroids_array = None\n",
    "    self.vertices = None\n",
    "    self.arestas = None\n",
    "    self.faces = None\n",
    "    self.nos_na_proxima_hierarquia = None\n",
    "    self.conexoes_na_proxima_hierarquia = None\n",
    "\n",
    "  def set_nodes(self):\n",
    "    nos = np.array([[1, 2, 3]])\n",
    "    with h5py.File(self.nodes_file, 'r') as file:\n",
    "        grupos = []\n",
    "        for group in file.keys():\n",
    "            if(\"entity\" in group):\n",
    "              grupos.append(group)\n",
    "\n",
    "        for grupo_nome in grupos:\n",
    "          grupo = file[grupo_nome]\n",
    "          for dataset in grupo.keys():\n",
    "              dados = []\n",
    "              try:\n",
    "                dados = grupo[dataset][:]\n",
    "                dados = np.round(dados, 6)\n",
    "                nos = np.concatenate((nos, dados))\n",
    "              except:\n",
    "                dados = grupo[dataset]\n",
    "        nos = nos[1:]\n",
    "        self.nodes = nos\n",
    "\n",
    "  def set_connectivities(self):\n",
    "    conexao = np.array([[1,2,3,4]])\n",
    "    with h5py.File(self.connectivities_file, 'r') as file:\n",
    "        grupos = []\n",
    "        for group in file.keys():\n",
    "            if(\"entity\" in group):\n",
    "              grupos.append(group)\n",
    "\n",
    "        for grupo_nome in grupos:\n",
    "          grupo = file[grupo_nome]\n",
    "          for dataset in grupo.keys():\n",
    "              dados = []\n",
    "              try:\n",
    "                dados = grupo[dataset][:]\n",
    "                if(isinstance(dados[0], np.ndarray)):\n",
    "                  if(len(dados[0]) == 4):\n",
    "                    conexao = np.concatenate((conexao, dados))\n",
    "              except:\n",
    "                dados = grupo[dataset]\n",
    "        conexao = conexao[1:]\n",
    "        self.connectivities = conexao\n",
    "\n",
    "  def get_centroids(self):\n",
    "    connectivities = np.array(self.connectivities) - 1  \n",
    "    nodes = np.array(self.nodes)\n",
    "\n",
    "    coords = nodes[connectivities]\n",
    "    \n",
    "    sum_coords = np.sum(coords, axis=1)\n",
    "    centroids = sum_coords / 4.0\n",
    "    \n",
    "    centroids_rounded = np.round(centroids, 6)\n",
    "    \n",
    "    indices = np.arange(len(centroids)).reshape(-1, 1)\n",
    "    centroids_with_indices = np.hstack((centroids_rounded, indices))\n",
    "    \n",
    "    self.centroids = centroids_with_indices\n",
    "    self.coords_centroids = coords\n",
    "\n",
    "  def sort_centroids(self, centroids):\n",
    "    centroids = np.asarray(centroids)\n",
    "    indices_ordenados = np.lexsort((centroids[:, 1], centroids[:, 0]))\n",
    "    return centroids[indices_ordenados]\n",
    "\n",
    "\n",
    "  def get_dimensions(self):\n",
    "    error = 0.0005\n",
    "    firstX = self.centroids[0, 0]\n",
    "    firstY = self.centroids[0, 1]\n",
    "\n",
    "    nHorizontal = np.sum((self.centroids[:, 0] >= firstX - error) & (self.centroids[:, 0] <= firstX + error))\n",
    "\n",
    "    nVertical = np.sum((self.centroids[:, 1] >= firstY - error) & (self.centroids[:, 1] <= firstY + error))\n",
    "\n",
    "    self.nHorizontal = nHorizontal\n",
    "    self.nVertical = nVertical\n",
    "\n",
    "  def divide_mesh(self, centroids, H, V, nX, nY):\n",
    "    if((H % nX != 0) or (V % nY) != 0):\n",
    "      print(\"Os valores H e V não são múltiplos de nX e nY\")\n",
    "      return\n",
    "\n",
    "    coord_grossa = np.empty(centroids.shape[0], dtype=object)\n",
    "\n",
    "    grossa_centroids_array = np.empty((H // nX) * (V // nY), dtype=object)\n",
    "    grossa_centroids_array[:] = [np.array([], dtype=int) for _ in range((H // nX) * (V // nY))]\n",
    "\n",
    "    vertices = np.zeros((H // nX) * (V // nY), dtype=int)\n",
    "    \n",
    "    arestas = np.empty((H // nX) * (V // nY), dtype=object)\n",
    "    faces = np.empty((H // nX) * (V // nY), dtype=object)\n",
    "    \n",
    "    arestas[:] = [np.array([], dtype=int) for _ in range((H // nX) * (V // nY))]\n",
    "    faces[:] = [np.array([], dtype=int) for _ in range((H // nX) * (V // nY))]\n",
    "\n",
    "    ja_foi_mapeado = set()\n",
    "\n",
    "    # Cria uma grade de valores de i e j\n",
    "    i_vals = np.arange(H)\n",
    "    j_vals = np.arange(V)\n",
    "    i_grid, j_grid = np.meshgrid(i_vals, j_vals, indexing='ij')\n",
    "    \n",
    "    # Calcula indX e indY vetorizados\n",
    "    indX = i_grid // nX\n",
    "    indY = j_grid // nY\n",
    "    \n",
    "    # Calcula os nós\n",
    "    nodes = i_grid * V + j_grid\n",
    "    \n",
    "    # Atribui a coordenada grossa para cada nó\n",
    "    coord_grossa[nodes.flatten()] = list(zip(indX.flatten(), indY.flatten()))\n",
    "    \n",
    "    # Prepara grossa_centroids_array usando uma lista de compreensão\n",
    "    grossa_centroids_array = np.empty((H // nX) * (V // nY), dtype=object)\n",
    "    grossa_centroids_array[:] = [np.array([], dtype=int) for _ in range((H // nX) * (V // nY))]\n",
    "    \n",
    "    # Atualiza grossa_centroids_array com os nós, de forma vetorizada\n",
    "    for indX_val, indY_val, node in zip(indX.flatten(), indY.flatten(), nodes.flatten()):\n",
    "        idx = indX_val * (V // nY) + indY_val\n",
    "        grossa_centroids_array[idx] = np.append(grossa_centroids_array[idx], node)\n",
    "          \n",
    "    # Calcula x e y para cada índice\n",
    "    indices = np.arange((H // nX) * (V // nY))\n",
    "    x = indices // (V // nY)\n",
    "    y = indices % (V // nY)\n",
    "    \n",
    "    # Calcula os vértices para cada condição\n",
    "    vertices[(x == 0) & (y == 0)] = [grossa_centroids_array[i][0] for i in indices[(x == 0) & (y == 0)]]\n",
    "    vertices[(x == (H // nX - 1)) & (y == 0)] = [grossa_centroids_array[i][-1] for i in indices[(x == (H // nX - 1)) & (y == 0)]]\n",
    "    vertices[(x == 0) & (y == (V // nY - 1))] = [grossa_centroids_array[i][-nY] for i in indices[(x == 0) & (y == (V // nY - 1))]]\n",
    "    vertices[(x == (H // nX - 1)) & (y == (V // nY - 1))] = [grossa_centroids_array[i][-1] for i in indices[(x == (H // nX - 1)) & (y == (V // nY - 1))]]\n",
    "    vertices[(x == 0) & (y > 0) & (y < (V // nY - 1))] = [grossa_centroids_array[i][nY // 2] for i in indices[(x == 0) & (y > 0) & (y < (V // nY - 1))]]\n",
    "    vertices[(y == (V // nY - 1)) & (x > 0) & (x < (H // nX - 1))] = [grossa_centroids_array[i][nY * (nX // 2 + 1) - 1] for i in indices[(y == (V // nY - 1)) & (x > 0) & (x < (H // nX - 1))]]\n",
    "    vertices[(y == 0) & (x > 0) & (x < (H // nX - 1))] = [grossa_centroids_array[i][nY * (nX // 2)] for i in indices[(y == 0) & (x > 0) & (x < (H // nX - 1))]]\n",
    "    vertices[(x == (H // nX - 1)) & (y > 0) & (y < (V // nY - 1))] = [grossa_centroids_array[i][-2] for i in indices[(x == (H // nX - 1)) & (y > 0) & (y < (V // nY - 1))]]\n",
    "    vertices[(x > 0) & (x < (H // nX - 1)) & (y > 0) & (y < (V // nY - 1))] = [grossa_centroids_array[i][(nX * nY) // 2] for i in indices[(x > 0) & (x < (H // nX - 1)) & (y > 0) & (y < (V // nY - 1))]]\n",
    "    \n",
    "    # Adiciona valores ao conjunto\n",
    "    ja_foi_mapeado = set(vertices)\n",
    "\n",
    "    # Preciso pegar as arestas\n",
    "    # Borda lateral esquerda\n",
    "    for index in range(0, V):\n",
    "      if index in ja_foi_mapeado:\n",
    "        continue\n",
    "      ja_foi_mapeado.add(index)\n",
    "      pertence_na_grossa = coord_grossa[index]\n",
    "      index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "      arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([index])))\n",
    "\n",
    "\n",
    "    # Borda inferior\n",
    "    for index in range(0, H*V, V):\n",
    "      if index in ja_foi_mapeado:\n",
    "        continue\n",
    "      ja_foi_mapeado.add(index)\n",
    "      pertence_na_grossa = coord_grossa[index]\n",
    "      index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "      arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([index])))\n",
    "\n",
    "\n",
    "    # Borda superior\n",
    "    for index in range(V-1, H*V, V):\n",
    "      if index in ja_foi_mapeado:\n",
    "        continue\n",
    "      ja_foi_mapeado.add(index)\n",
    "      pertence_na_grossa = coord_grossa[index]\n",
    "      index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "      arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([index])))\n",
    "     \n",
    "\n",
    "    # Borda lateral direita\n",
    "    for index in range(V*(H-1), H*V):\n",
    "      if index in ja_foi_mapeado:\n",
    "        continue\n",
    "      ja_foi_mapeado.add(index)\n",
    "      pertence_na_grossa = coord_grossa[index]\n",
    "      index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "      arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([index])))\n",
    "\n",
    "    \n",
    "    #Para cada vertice não-borda, só andar pros 4 lados até atingir um vértice.\n",
    "\n",
    "    todos_vertices = vertices\n",
    "    todos_vertices_nao_borda = []\n",
    "\n",
    "    for index in range(0, (H // nX) * (V // nY)):\n",
    "      i = index // ((V // nY))\n",
    "      j = index % ((V // nY))\n",
    "      if(i == 0 or i == (H//nX - 1) or j == 0 or j == (V // nY - 1)):\n",
    "        continue\n",
    "      todos_vertices_nao_borda.append(vertices[index])\n",
    "\n",
    "    todos_vertices_nao_borda = np.array(todos_vertices_nao_borda)\n",
    "        \n",
    "\n",
    "    for index in todos_vertices_nao_borda:\n",
    "      # Ando para cima até encontrar um vertice.\n",
    "      daVezU = index + 1\n",
    "\n",
    "      while (daVezU not in ja_foi_mapeado):\n",
    "        pertence_na_grossa = coord_grossa[daVezU]\n",
    "        index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "        arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([daVezU])))\n",
    "        ja_foi_mapeado.add(daVezU)\n",
    "        daVezU += 1\n",
    "\n",
    "      # Ando para baixo\n",
    "      daVezD = index - 1\n",
    "      while (daVezD not in ja_foi_mapeado):\n",
    "        pertence_na_grossa = coord_grossa[daVezD]\n",
    "        index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "        arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([daVezD])))\n",
    "        ja_foi_mapeado.add(daVezD)\n",
    "        daVezD -= 1\n",
    "\n",
    "      # Ando para a direita\n",
    "      daVezR = index + V\n",
    "      while (daVezR not in ja_foi_mapeado):\n",
    "        pertence_na_grossa = coord_grossa[daVezR]\n",
    "        index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "        arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([daVezR])))\n",
    "        ja_foi_mapeado.add(daVezR)\n",
    "        daVezR += V\n",
    "\n",
    "      # Ando para a esquerda\n",
    "      daVezL = index - V\n",
    "      while (daVezL not in ja_foi_mapeado):\n",
    "        pertence_na_grossa = coord_grossa[daVezL]\n",
    "        index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "        arestas[index_na_grossa] = np.concatenate((arestas[index_na_grossa], np.array([daVezL])))\n",
    "        ja_foi_mapeado.add(daVezL)\n",
    "        daVezL -= V\n",
    "\n",
    "    for index in range(0, V*H):\n",
    "      if(index not in ja_foi_mapeado):\n",
    "        ja_foi_mapeado.add(index)\n",
    "        pertence_na_grossa = coord_grossa[index]\n",
    "        index_na_grossa = pertence_na_grossa[0]*((V // nY)) + pertence_na_grossa[1]\n",
    "        faces[index_na_grossa] = np.concatenate((faces[index_na_grossa], np.array([index])))\n",
    "\n",
    "    self.coord_grossa = coord_grossa\n",
    "    self.grossa_centroids_array = grossa_centroids_array\n",
    "    self.vertices = vertices\n",
    "    self.arestas = arestas\n",
    "    self.faces = faces\n",
    "    self.nX = nX\n",
    "    self.nY = nY\n",
    "\n",
    "  def proxima_hierarquia(self):\n",
    "    sz = len(self.coord_grossa)\n",
    "    \n",
    "    indices = np.arange(sz)\n",
    "    coord_grossa_array = np.array(self.coord_grossa)\n",
    "    \n",
    "    chaves_unicas = np.unique(coord_grossa_array)\n",
    "    num_chaves = len(chaves_unicas)\n",
    "    \n",
    "    nos_por_index_grossa = [np.array([]) for _ in range(num_chaves)]\n",
    "    \n",
    "    for i, chave in enumerate(chaves_unicas):\n",
    "        chave_array = np.array(chave, dtype=object)  # Garante que a chave seja tratada como array NumPy\n",
    "        indices_correspondentes = np.where(coord_grossa_array == chave_array)[0]\n",
    "        nos_por_index_grossa[i] = indices_correspondentes\n",
    "    \n",
    "    nos_na_proxima_hierarquia = np.array([[1, 2, 3]])\n",
    "    conexoes_na_proxima_hierarquia = np.array([[1, 2, 3, 4]])\n",
    "    index = 1\n",
    "    \n",
    "    # Processa cada grupo de índices\n",
    "    for elto in nos_por_index_grossa:\n",
    "        tam = len(elto)\n",
    "        if tam < self.nY:\n",
    "            continue  # Evita processamento se não houver índices suficientes\n",
    "\n",
    "        esquerda_inferior = elto[0]\n",
    "        esquerda_superior = elto[self.nY - 1]\n",
    "        direita_inferior = elto[tam - self.nY]\n",
    "        direita_superior = elto[tam - 1]\n",
    "\n",
    "        self.coords_centroids[esquerda_inferior] = self.sort_centroids(self.coords_centroids[esquerda_inferior])\n",
    "        self.coords_centroids[esquerda_superior] = self.sort_centroids(self.coords_centroids[esquerda_superior])\n",
    "        self.coords_centroids[direita_inferior] = self.sort_centroids(self.coords_centroids[direita_inferior])\n",
    "        self.coords_centroids[direita_superior] = self.sort_centroids(self.coords_centroids[direita_superior])\n",
    "\n",
    "        nos_na_proxima_hierarquia = np.concatenate((nos_na_proxima_hierarquia, [self.coords_centroids[esquerda_inferior][0]]))\n",
    "        nos_na_proxima_hierarquia = np.concatenate((nos_na_proxima_hierarquia, [self.coords_centroids[esquerda_superior][1]]))\n",
    "        nos_na_proxima_hierarquia = np.concatenate((nos_na_proxima_hierarquia, [self.coords_centroids[direita_inferior][2]]))\n",
    "        nos_na_proxima_hierarquia = np.concatenate((nos_na_proxima_hierarquia, [self.coords_centroids[direita_superior][3]]))\n",
    "\n",
    "        conexoes_na_proxima_hierarquia = np.concatenate((conexoes_na_proxima_hierarquia, [[index, index + 1, index + 3, index + 2]]))\n",
    "        index += 4\n",
    "\n",
    "    # Remove o primeiro elemento, que era inicializado\n",
    "    nos_na_proxima_hierarquia = nos_na_proxima_hierarquia[1:]\n",
    "    conexoes_na_proxima_hierarquia = conexoes_na_proxima_hierarquia[1:]\n",
    "\n",
    "    self.nos_na_proxima_hierarquia = nos_na_proxima_hierarquia\n",
    "    self.conexoes_na_proxima_hierarquia = conexoes_na_proxima_hierarquia\n",
    "\n",
    "  def plot_mesh(self):\n",
    "\n",
    "    plt.figure(figsize=(40, 32))\n",
    "    squares = self.connectivities\n",
    "\n",
    "    # Print dos nos\n",
    "    for i, (x, y, z) in enumerate(self.nodes):\n",
    "        plt.plot(x, y, 'bo')\n",
    "\n",
    "    # Print das conexões\n",
    "    for sq in squares:\n",
    "      p1 = self.nodes[sq[0] - 1]\n",
    "      p2 = self.nodes[sq[1] - 1]\n",
    "      p3 = self.nodes[sq[2] - 1]\n",
    "      p4 = self.nodes[sq[3] - 1]\n",
    "      plt.plot([p1[0], p2[0], p3[0], p4[0], p1[0]], [p1[1], p2[1], p3[1], p4[1], p1[1]], 'red')\n",
    "\n",
    "    for i, (x, y, z, index) in enumerate(self.centroids):\n",
    "      plt.plot(x, y, 'bo')  # Plot do ponto\n",
    "\n",
    "      # Adicionando o número do nó ao lado do ponto\n",
    "      plt.text(x, y, str(i), color='black', fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Plotagem da malha')\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "  def process(self, nX, nY, H, V):\n",
    "    self.set_nodes()\n",
    "    self.set_connectivities()\n",
    "    self.get_centroids()\n",
    "    self.centroids = self.sort_centroids(self.centroids)\n",
    "    self.nHorizontal = H\n",
    "    self.nVertical = V\n",
    "    self.divide_mesh(self.centroids, self.nHorizontal, self.nVertical, nX, nY)\n",
    "    self.proxima_hierarquia()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
